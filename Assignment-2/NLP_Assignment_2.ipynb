{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# Assignment 2 on Natural Language Processing\n",
    "\n",
    "### Date : 15th Sept, 2020\n",
    "\n",
    "### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stk58juYkzEr"
   },
   "source": [
    "**Dataset:** \n",
    "\n",
    " Use the text file provided along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rT6byv49kdmo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = r\"C:\\class work\\fourth year\\sem7\\NLP\"\n",
    "filename = \"corpus.txt\"\n",
    "path_to_file = os.path.join(base_path, filename)\n",
    "fd = open(path_to_file ,encoding=\"utf8\")\n",
    "text = fd.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRGqKaDn1pJy"
   },
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1OtHn6B1oc2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "text = text.replace('_', '')\n",
    "text = text.split('\\n')\n",
    "for i in range(len(text)):\n",
    "    text[i] = re.sub(r'[^\\w\\s]', ' ', text[i])\n",
    "    text[i] = text[i].lower()\n",
    "    text[i] = re.sub(r'\\w*\\d\\w*', ' ', text[i])\n",
    "#    print(text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDL7yfpXkMRS"
   },
   "source": [
    "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
    "\n",
    "1. **Create the following language models** on the training corpus: <br>\n",
    "    i.   Unigram <br>\n",
    "    ii.  Bigram <br>\n",
    "    iii. Trigram <br>\n",
    "    iv.  Fourgram <br>\n",
    "\n",
    "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
    "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3oIulBikPua",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten unigrams are : \n",
      "the \t 1643\n",
      "and \t 872\n",
      "to \t 729\n",
      "a \t 632\n",
      "it \t 595\n",
      "she \t 553\n",
      "i \t 545\n",
      "of \t 514\n",
      "said \t 462\n",
      "you \t 411\n",
      "\n",
      "Top ten bigrams are : \n",
      "('said', 'the') \t 209\n",
      "('of', 'the') \t 133\n",
      "('said', 'alice') \t 116\n",
      "('in', 'a') \t 97\n",
      "('and', 'the') \t 82\n",
      "('in', 'the') \t 79\n",
      "('it', 'was') \t 76\n",
      "('the', 'queen') \t 72\n",
      "('to', 'the') \t 69\n",
      "('the', 'king') \t 62\n",
      "\n",
      "Top ten trigrams are : \n",
      "('the', 'mock', 'turtle') \t 53\n",
      "('i', 'don', 't') \t 31\n",
      "('the', 'march', 'hare') \t 30\n",
      "('said', 'the', 'king') \t 29\n",
      "('the', 'white', 'rabbit') \t 21\n",
      "('said', 'the', 'hatter') \t 21\n",
      "('said', 'to', 'herself') \t 19\n",
      "('said', 'the', 'mock') \t 19\n",
      "('said', 'the', 'caterpillar') \t 18\n",
      "('she', 'went', 'on') \t 17\n",
      "\n",
      "Top ten fourgrams are : \n",
      "('said', 'the', 'mock', 'turtle') \t 19\n",
      "('she', 'said', 'to', 'herself') \t 16\n",
      "('a', 'minute', 'or', 'two') \t 11\n",
      "('you', 'won', 't', 'you') \t 10\n",
      "('said', 'the', 'march', 'hare') \t 8\n",
      "('will', 'you', 'won', 't') \t 8\n",
      "('said', 'alice', 'in', 'a') \t 7\n",
      "('i', 'don', 't', 'know') \t 7\n",
      "('as', 'well', 'as', 'she') \t 6\n",
      "('well', 'as', 'she', 'could') \t 6\n"
     ]
    }
   ],
   "source": [
    "#Write code\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "unigrams=[]\n",
    "bigrams=[]\n",
    "trigrams=[]\n",
    "fourgrams=[]\n",
    "\n",
    "for content in (text):\n",
    "    token = nltk.word_tokenize(content)\n",
    "    unigrams.extend(token)     \n",
    "    bigrams.extend(ngrams(token,2))\n",
    "    trigrams.extend(ngrams(token,3))\n",
    "    fourgrams.extend(ngrams(token,4))\n",
    "    \n",
    "fdist = nltk.FreqDist(unigrams)\n",
    "print('Top ten unigrams are : ')\n",
    "for k in fdist.most_common(10):\n",
    "    print(k[0],'\\t',k[1])\n",
    "print()\n",
    "\n",
    "fdist = nltk.FreqDist(bigrams)\n",
    "print('Top ten bigrams are : ')\n",
    "for k in fdist.most_common(10):\n",
    "    print(k[0],'\\t',k[1])\n",
    "print()\n",
    "\n",
    "fdist = nltk.FreqDist(trigrams)\n",
    "print('Top ten trigrams are : ')\n",
    "for k in fdist.most_common(10):\n",
    "    print(k[0],'\\t',k[1])\n",
    "print()\n",
    "\n",
    "fdist = nltk.FreqDist(fourgrams)\n",
    "print('Top ten fourgrams are : ')\n",
    "for k in fdist.most_common(10):\n",
    "    print(k[0],'\\t',k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vARsvSfynePr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten unigrams after stopwords removing are : \n",
      "said \t 462\n",
      "alice \t 397\n",
      "little \t 128\n",
      "one \t 104\n",
      "know \t 87\n",
      "like \t 85\n",
      "would \t 83\n",
      "went \t 83\n",
      "could \t 77\n",
      "queen \t 75\n",
      "\n",
      "Top ten bigrams after stopword removing are : \n",
      "('said', 'the') \t 209\n",
      "('said', 'alice') \t 116\n",
      "('the', 'queen') \t 72\n",
      "('the', 'king') \t 62\n",
      "('a', 'little') \t 59\n",
      "('mock', 'turtle') \t 56\n",
      "('the', 'mock') \t 53\n",
      "('the', 'gryphon') \t 53\n",
      "('the', 'hatter') \t 52\n",
      "('went', 'on') \t 48\n",
      "\n",
      "Top ten trigrams after stopwords removing are : \n",
      "('the', 'mock', 'turtle') \t 53\n",
      "('the', 'march', 'hare') \t 30\n",
      "('said', 'the', 'king') \t 29\n",
      "('the', 'white', 'rabbit') \t 21\n",
      "('said', 'the', 'hatter') \t 21\n",
      "('said', 'to', 'herself') \t 19\n",
      "('said', 'the', 'mock') \t 19\n",
      "('said', 'the', 'caterpillar') \t 18\n",
      "('she', 'went', 'on') \t 17\n",
      "('she', 'said', 'to') \t 17\n",
      "\n",
      "Top ten fourgrams after stopword removing are : \n",
      "('said', 'the', 'mock', 'turtle') \t 19\n",
      "('she', 'said', 'to', 'herself') \t 16\n",
      "('a', 'minute', 'or', 'two') \t 11\n",
      "('said', 'the', 'march', 'hare') \t 8\n",
      "('said', 'alice', 'in', 'a') \t 7\n",
      "('i', 'don', 't', 'know') \t 7\n",
      "('as', 'well', 'as', 'she') \t 6\n",
      "('well', 'as', 'she', 'could') \t 6\n",
      "('in', 'a', 'great', 'hurry') \t 6\n",
      "('in', 'a', 'tone', 'of') \t 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "#print top 10 unigrams, bigrams after removing stopwords\n",
    "uni_processed = [p for p in unigrams if p not in stopwords]\n",
    "fdist = nltk.FreqDist(uni_processed)\n",
    "print('Top ten unigrams after stopwords removing are : ')\n",
    "for k in fdist.most_common(10):\n",
    "    print(k[0],'\\t',k[1])\n",
    "print()\n",
    "\n",
    "bi_processed = []\n",
    "for p in bigrams:\n",
    "    fl = 0\n",
    "    for q in p:\n",
    "        if q not in stopwords:\n",
    "            fl = 1\n",
    "            break\n",
    "    if fl>0:\n",
    "        bi_processed.append(p)\n",
    "fdist = nltk.FreqDist(bi_processed)\n",
    "print('Top ten bigrams after stopword removing are : ')\n",
    "for k in fdist.most_common(10):\n",
    "    print(k[0],'\\t',k[1])\n",
    "print()\n",
    "\n",
    "tri_processed = []\n",
    "for p in trigrams:\n",
    "    fl = 0\n",
    "    for q in p:\n",
    "        if q not in stopwords:\n",
    "            fl = 1\n",
    "            break\n",
    "    if fl>0:\n",
    "        tri_processed.append(p)\n",
    "fdist = nltk.FreqDist(tri_processed)\n",
    "print('Top ten trigrams after stopwords removing are : ')\n",
    "for k in fdist.most_common(10):\n",
    "    print(k[0],'\\t',k[1])\n",
    "print()\n",
    "\n",
    "four_processed = []\n",
    "for p in fourgrams:\n",
    "    fl = 0\n",
    "    for q in p:\n",
    "        if q not in stopwords:\n",
    "            fl = 1\n",
    "            break\n",
    "    if fl>0:\n",
    "        four_processed.append(p)\n",
    "fdist = nltk.FreqDist(four_processed)\n",
    "print('Top ten fourgrams after stopword removing are : ')\n",
    "for k in fdist.most_common(10):\n",
    "    print(k[0],'\\t',k[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioc1xNjmnim-"
   },
   "source": [
    "# Applying Smoothing\n",
    "\n",
    "\n",
    "Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n",
    "\n",
    "> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n",
    "\n",
    "N: Total number of N-grams <br>\n",
    "V: Number of unique N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grh4sO0Yns4V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten unigrams after smoothing are : \n",
      "the \t 0.05498143874786796\n",
      "and \t 0.029196347948229156\n",
      "to \t 0.02441389920069563\n",
      "a \t 0.021169860539781277\n",
      "it \t 0.019932443730978897\n",
      "she \t 0.01852780843450052\n",
      "i \t 0.018260258854218923\n",
      "of \t 0.01722350423062774\n",
      "said \t 0.015484431958797364\n",
      "you \t 0.013778803384502191\n",
      "\n",
      "Top ten bigrams after smoothing are : \n",
      "('said', 'the') \t 0.0052337752965806\n",
      "('of', 'the') \t 0.0033396470940085733\n",
      "('said', 'alice') \t 0.00291596052238062\n",
      "('in', 'a') \t 0.0024424284717376133\n",
      "('and', 'the') \t 0.0020685873791247136\n",
      "('in', 'the') \t 0.0019938191606021336\n",
      "('it', 'was') \t 0.0019190509420795533\n",
      "('the', 'queen') \t 0.0018193599840494467\n",
      "('to', 'the') \t 0.0017445917655268667\n",
      "('the', 'king') \t 0.00157013258897418\n",
      "\n",
      "Top ten trigrams after smoothing are : \n",
      "('the', 'mock', 'turtle') \t 0.0011510668684586362\n",
      "('i', 'don', 't') \t 0.00068211369982734\n",
      "('the', 'march', 'hare') \t 0.0006607976467077356\n",
      "('said', 'the', 'king') \t 0.0006394815935881312\n",
      "('the', 'white', 'rabbit') \t 0.00046895316863129624\n",
      "('said', 'the', 'hatter') \t 0.00046895316863129624\n",
      "('said', 'to', 'herself') \t 0.0004263210623920875\n",
      "('said', 'the', 'mock') \t 0.0004263210623920875\n",
      "('said', 'the', 'caterpillar') \t 0.0004050050092724831\n",
      "('she', 'went', 'on') \t 0.00038368895615287875\n",
      "\n",
      "Top ten fourgrams after smoothing are : \n",
      "('said', 'the', 'mock', 'turtle') \t 0.0004183400267737617\n",
      "('she', 'said', 'to', 'herself') \t 0.00035558902275769743\n",
      "('a', 'minute', 'or', 'two') \t 0.000251004016064257\n",
      "('you', 'won', 't', 'you') \t 0.00023008701472556893\n",
      "('said', 'the', 'march', 'hare') \t 0.00018825301204819278\n",
      "('will', 'you', 'won', 't') \t 0.00018825301204819278\n",
      "('said', 'alice', 'in', 'a') \t 0.00016733601070950468\n",
      "('i', 'don', 't', 'know') \t 0.00016733601070950468\n",
      "('as', 'well', 'as', 'she') \t 0.0001464190093708166\n",
      "('well', 'as', 'she', 'could') \t 0.0001464190093708166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#You are to perform Add-1 smoothing here:\n",
    "fdist = nltk.FreqDist(unigrams)\n",
    "n = len(unigrams)\n",
    "v = len(set(unigrams))\n",
    "newdist1 = fdist.copy()\n",
    "for x in set(unigrams):\n",
    "    c = fdist[x]\n",
    "    newdist1[x] = (c+1)/(n+v)\n",
    "print('Top ten unigrams after smoothing are : ')\n",
    "for k in newdist1.most_common(10):\n",
    "    print(k[0],'\\t',k[1])\n",
    "print()\n",
    "\n",
    "#write similar code for bigram, trigram and fourgrams\n",
    "fdist = nltk.FreqDist(bigrams)\n",
    "n = len(bigrams)\n",
    "v = len(set(bigrams))\n",
    "newdist2 = fdist.copy()\n",
    "for x in set(bigrams):\n",
    "    c = fdist[x]\n",
    "    newdist2[x] = (c+1)/(n+v)\n",
    "print('Top ten bigrams after smoothing are : ')\n",
    "for k in newdist2.most_common(10):\n",
    "    print(k[0],'\\t',k[1])\n",
    "print()\n",
    "\n",
    "fdist = nltk.FreqDist(trigrams)\n",
    "n = len(trigrams)\n",
    "v = len(set(trigrams))\n",
    "newdist3 = fdist.copy()\n",
    "for x in set(trigrams):\n",
    "    c = fdist[x]\n",
    "    newdist3[x] = (c+1)/(n+v)\n",
    "print('Top ten trigrams after smoothing are : ')\n",
    "for k in newdist3.most_common(10):\n",
    "    print(k[0],'\\t',k[1])\n",
    "print()\n",
    "\n",
    "fdist = nltk.FreqDist(fourgrams)\n",
    "n = len(fourgrams)\n",
    "v = len(set(fourgrams))\n",
    "newdist4 = fdist.copy()\n",
    "for x in set(fourgrams):\n",
    "    c = fdist[x]\n",
    "    newdist4[x] = (c+1)/(n+v)\n",
    "print('Top ten fourgrams after smoothing are : ')\n",
    "for k in newdist4.most_common(10):\n",
    "    print(k[0],'\\t',k[1])\n",
    "print()\n",
    "#Print top 10 unigram, bigram, trigram, fourgram after smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0GL40mQmmt4"
   },
   "source": [
    "### Predict the next word using statistical language modelling\n",
    "\n",
    "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n",
    "\n",
    "For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n",
    "For example, for the string 'He looked very' the answers can be as below: \n",
    ">     (1) 'He looked very' *anxiouxly* \n",
    ">     (2) 'He looked very' *uncomfortable* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBWKo5_Fmnbg"
   },
   "outputs": [],
   "source": [
    "str1 = 'after that alice said the'\n",
    "str2 = 'alice felt so desperate that she was'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ext_nVn2mvZt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram prediction of string 1:\n",
      "after that alice said the queen\n",
      "after that alice said the king\n",
      "after that alice said the mock\n",
      "after that alice said the gryphon\n",
      "after that alice said the hatter\n",
      "\n",
      "Trigram prediction of string 1:\n",
      "after that alice said the king\n",
      "after that alice said the hatter\n",
      "after that alice said the mock\n",
      "after that alice said the caterpillar\n",
      "after that alice said the gryphon\n",
      "\n",
      "Fourgram prediction of string 1:\n",
      "NULL\n",
      "\n",
      "Bigram prediction of string 2:\n",
      "alice felt so desperate that she was a\n",
      "alice felt so desperate that she was the\n",
      "alice felt so desperate that she was not\n",
      "alice felt so desperate that she was going\n",
      "alice felt so desperate that she was that\n",
      "\n",
      "Trigram prediction of string 2:\n",
      "alice felt so desperate that she was now\n",
      "alice felt so desperate that she was quite\n",
      "alice felt so desperate that she was a\n",
      "alice felt so desperate that she was walking\n",
      "alice felt so desperate that she was looking\n",
      "\n",
      "Fourgram prediction of string 2:\n",
      "alice felt so desperate that she was now\n",
      "alice felt so desperate that she was dozing\n",
      "alice felt so desperate that she was walking\n",
      "alice felt so desperate that she was ready\n",
      "alice felt so desperate that she was in\n"
     ]
    }
   ],
   "source": [
    "w1 = str1.split()\n",
    "w1.reverse()\n",
    "print('Bigram prediction of string 1:')\n",
    "cnt = 0\n",
    "for p in newdist2.most_common():\n",
    "    ngram = p[0]\n",
    "    if ngram[0]==w1[0]:\n",
    "        print(str1,ngram[1])\n",
    "        cnt = cnt + 1\n",
    "    if cnt==5:\n",
    "        break\n",
    "\n",
    "print('\\nTrigram prediction of string 1:')\n",
    "cnt = 0\n",
    "for p in newdist3.most_common():\n",
    "    ngram = p[0]\n",
    "    if ngram[1]==w1[0] and ngram[0]==w1[1]:\n",
    "        print(str1,ngram[2])\n",
    "        cnt = cnt + 1\n",
    "    if cnt==5:\n",
    "        break\n",
    "\n",
    "print('\\nFourgram prediction of string 1:')\n",
    "cnt = 0\n",
    "for p in newdist4.most_common():\n",
    "    ngram = p[0]\n",
    "    if ngram[2]==w1[0] and ngram[1]==w1[1] and ngram[0]==w1[2]:\n",
    "        print(str1,ngram[3])\n",
    "        cnt = cnt + 1\n",
    "    if cnt==5:\n",
    "        break\n",
    "if cnt==0:\n",
    "    print('NULL')\n",
    "    \n",
    "w1 = str2.split()\n",
    "w1.reverse()\n",
    "\n",
    "print('\\nBigram prediction of string 2:')\n",
    "cnt = 0\n",
    "for p in newdist2.most_common():\n",
    "    ngram = p[0]\n",
    "    if ngram[0]==w1[0]:\n",
    "        print(str2,ngram[1])\n",
    "        cnt = cnt + 1\n",
    "    if cnt==5:\n",
    "        break\n",
    "\n",
    "print('\\nTrigram prediction of string 2:')\n",
    "cnt = 0\n",
    "for p in newdist3.most_common():\n",
    "    ngram = p[0]\n",
    "    if ngram[1]==w1[0] and ngram[0]==w1[1]:\n",
    "        print(str2,ngram[2])\n",
    "        cnt = cnt + 1\n",
    "    if cnt==5:\n",
    "        break\n",
    "\n",
    "print('\\nFourgram prediction of string 2:')\n",
    "cnt = 0\n",
    "for p in newdist4.most_common():\n",
    "    ngram = p[0]\n",
    "    if ngram[2]==w1[0] and ngram[1]==w1[1] and ngram[0]==w1[2]:\n",
    "        print(str2,ngram[3])\n",
    "        cnt = cnt + 1\n",
    "    if cnt==5:\n",
    "        break\n",
    "if cnt==0:\n",
    "    print('NULL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
