{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fk0evCZ7U9WO"
   },
   "source": [
    "# **Assignment 1 on Natural Language Processing**\n",
    "\n",
    "### Date : 4th Sept, 2020\n",
    "\n",
    "#### Instructor : Prof. Sudeshna Sarkar\n",
    "\n",
    "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Il_b_LFKXi8t"
   },
   "source": [
    " # NLTK Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ss5CZjC2Xt0i"
   },
   "source": [
    "The [NLTK](https://www.nltk.org/) Python framework is generally used as an education and research tool. Tokenization, Stemming, Lemmatization, Punctuation, Character count, word count are some of these packages which will be discussed in this tutorial.\n",
    "\n",
    "**Installing Nltk** <br>\n",
    "Nltk can be installed using PIP or Conda package managers.For detailed installation instructions follow this [link](https://www.nltk.org/install.html).\n",
    "\n",
    "To ensure we are all on the same page, the coding environment will be in **python3**. We suggest downloading Anaconda3 and creating a separate environment to do this assignment. \n",
    "The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. \n",
    "The steps to install NLTK is available on the link: \n",
    "```bash\n",
    "sudo pip3 install nltk \n",
    "python3 \n",
    "nltk.download()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4txbU5-RlMv"
   },
   "source": [
    "**Note for Question and answers:**\n",
    "\n",
    "Write your answers to the point in the text box below labelled as **Answer here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52_aJRSqaHgC"
   },
   "source": [
    "# Tokenizing words and Sentences using Nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5_ElYaeaMbR"
   },
   "source": [
    "**Tokenization** is the process by which big quantity of text is divided into smaller parts called tokens. <br>It is crucial to understand the pattern in the text in order to perform various NLP tasks.These tokens are very useful for finding such patterns.<br>\n",
    "\n",
    "Natural Language toolkit has very important module tokenize which further comprises of sub-modules\n",
    "\n",
    "1. word tokenize\n",
    "2. sentence tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sby_OS3qZ_fz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "import nltk\n",
    "nltk.download('punkt') # For tokenizers\n",
    "nltk.download('inaugural') # For dataset\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ew9Aq5WHXSn-"
   },
   "outputs": [],
   "source": [
    "# Sample corpus.\n",
    "from nltk.corpus import inaugural,stopwords\n",
    "corpus = inaugural.raw('1789-Washington.txt')\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2wbXKzVW0GO"
   },
   "source": [
    "### **TASK**:\n",
    "\n",
    "For the given corpus, \n",
    "1. Print the number of sentences and tokens. \n",
    "2. Print the average number of tokens per sentence.\n",
    "3. Print the number of unique tokens\n",
    "4. Print the number of tokens after stopword removal using the stopwords from nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jrtu9HcHXFe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of sentences is : 23 and number of tokens is : 1537\n",
      "2. Avg number tokens per sentences is : 66.82608695652173\n",
      "3. Number of unique tokens is : 626\n",
      "4. Number of tokens after stopword removal is : 800\n"
     ]
    }
   ],
   "source": [
    " x = len(sent_tokenize(corpus))\n",
    "y = len(word_tokenize(corpus))\n",
    "z = len(set(word_tokenize(corpus)))\n",
    "print('1. Number of sentences is : '+str(x)+' and number of tokens is : '+str(y) )\n",
    "print('2. Avg number tokens per sentences is : '+str(y/x))\n",
    "print('3. Number of unique tokens is : '+ str(z))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in word_tokenize(corpus) if not w in stop_words] \n",
    "print('4. Number of tokens after stopword removal is : '+str(len(filtered_sentence)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UViYY9_3t2UE"
   },
   "source": [
    "# Stemming and Lemmatization with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g55XX9KDLgO7"
   },
   "source": [
    "**What is Stemming?** <br>\n",
    "Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.<br>\n",
    "Hence Stemming is a way to find the root word from any variations of respective word\n",
    "\n",
    "There are many stemmers provided by Nltk like **PorterStemmer**, **SnowballStemmer**, **LancasterStemmer**.<br>\n",
    "\n",
    "We will try and see differences between Porterstemmer and Snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SS4Ij__XLfTB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming using PorterStemmer\n",
      "grows  :  grow\n",
      "leaves  :  leav\n",
      "fairly  :  fairli\n",
      "cats  :  cat\n",
      "trouble  :  troubl\n",
      "misunderstanding  :  misunderstand\n",
      "friendships  :  friendship\n",
      "easily  :  easili\n",
      "rational  :  ration\n",
      "relational  :  relat\n",
      "\n",
      "\n",
      "Stemming using SnowballStemmer\n",
      "grows  :  grow\n",
      "leaves  :  leav\n",
      "fairly  :  fair\n",
      "cats  :  cat\n",
      "trouble  :  troubl\n",
      "misunderstanding  :  misunderstand\n",
      "friendships  :  friendship\n",
      "easily  :  easili\n",
      "rational  :  ration\n",
      "relational  :  relat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer # Note that SnowballStemmer has language as parameter.\n",
    "\n",
    "words = [\"grows\",\"leaves\",\"fairly\",\"cats\",\"trouble\",\"misunderstanding\",\"friendships\",\"easily\", \"rational\", \"relational\"]\n",
    "\n",
    "# TODO\n",
    "ps = PorterStemmer()\n",
    "ss = SnowballStemmer('english')\n",
    "\n",
    "# create an instance of both the stemmers and perform stemming on above words\n",
    "\n",
    "# TODO\n",
    "# Complete the function which takes a sentence/corpus and gets its stemmed version.\n",
    "def stemSentence(sentence=None):\n",
    "    print('Stemming using PorterStemmer')\n",
    "    for w in sentence: \n",
    "        print(w, \" : \", ps.stem(w))\n",
    "    print('\\n\\nStemming using SnowballStemmer')\n",
    "    for w in sentence: \n",
    "        print(w, \" : \", ss.stem(w))\n",
    "\n",
    "stemSentence(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JuE8CuDQSno"
   },
   "source": [
    "**What is Lemmatization?** <br>\n",
    "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as the lemma.<br>\n",
    "\n",
    "*The NLTK Lemmatization method is based on WorldNet's built-in morph function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noyl1YNsQp98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatize words in context of POS = n\n",
      "grows  :  grows\n",
      "leaves  :  leaf\n",
      "fairly  :  fairly\n",
      "cats  :  cat\n",
      "trouble  :  trouble\n",
      "running  :  running\n",
      "friendships  :  friendship\n",
      "easily  :  easily\n",
      "was  :  wa\n",
      "relational  :  relational\n",
      "has  :  ha\n",
      "\n",
      "\n",
      "Lemmatize words in context of POS = v\n",
      "grows  :  grow\n",
      "leaves  :  leave\n",
      "fairly  :  fairly\n",
      "cats  :  cat\n",
      "trouble  :  trouble\n",
      "running  :  run\n",
      "friendships  :  friendships\n",
      "easily  :  easily\n",
      "was  :  be\n",
      "relational  :  relational\n",
      "has  :  have\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') # Since Lemmatization method is based on WorldNet's built-in morph function.\n",
    "\n",
    "words = [\"grows\",\"leaves\",\"fairly\",\"cats\",\"trouble\",\"running\",\"friendships\",\"easily\", \"was\", \"relational\",\"has\"]\n",
    "\n",
    "#TODO\n",
    "# Create an instance of the Lemmatizer and perform Lemmatization on above words\n",
    "# You can also give Parts-of-speech(pos) to the Lemmatizer for example \"v\" (verb). Check the differences in the outputs.\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "#TODO\n",
    "# Complete the function which takes a sentence/corpus and gets its lemmatized version.\n",
    "def lemmatizeSentence(sentence=None):\n",
    "    print('Lemmatize words in context of POS = n')\n",
    "    for w in sentence: \n",
    "        print(w, \" : \", lm.lemmatize(w,pos=\"n\"))\n",
    "    print('\\n\\nLemmatize words in context of POS = v')\n",
    "    for w in sentence: \n",
    "        print(w, \" : \", lm.lemmatize(w,pos=\"v\"))\n",
    "lemmatizeSentence(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJW6HsycSAlU"
   },
   "source": [
    "**Question:** Give example of two words which have same stem but different lemma? Show the stem and lemma of both words in the code below \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcq6bUEaSAt1"
   },
   "source": [
    "**Answer here:**\n",
    "Lemma takes into consideration the context of words while normalizing them. Stemming operates without knowledge of context and usually usually refers to a crude heuristic process that chops off the ends of words.\\\n",
    "Eg: meeting and meet\\\n",
    "The lemma of \"meeting\" in context of noun is \"meeting\" itself and stem word of \"meeting\" is \"meet\" which is obtained by removing -ing.\n",
    "The lemma of \"meet\" in context of noun is \"meet\" itself and stem word of \"meet\" is \"meet\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8OtIEmFkGBM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem of meeting :  meet\n",
      "Stem of meet :  meet\n",
      "Lemma of meeting :  meeting\n",
      "Lemma of meet :  meet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import SnowballStemmer\n",
    "w1 = \"meeting\"\n",
    "w2 = \"meet\"\n",
    "ss = SnowballStemmer('english')\n",
    "print(\"Stem of\",w1, \": \", ss.stem(w1))\n",
    "print(\"Stem of\",w2, \": \", ss.stem(w2))\n",
    "print(\"Lemma of\",w1, \": \", lm.lemmatize(w1))\n",
    "print(\"Lemma of\",w2, \": \", lm.lemmatize(w2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0tz3SIGSA2b"
   },
   "source": [
    "**Question:** Write a comparison between stemming and lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aks8QaQ_SA_N"
   },
   "source": [
    "**Answer here:**\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "The two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "Another difference is that stemming operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech unlike lemmatization which considers context. \n",
    "\n",
    "It usually sufficient that related words map to the same stem,even if the stem is not in itself a valid root, while in lemmatisation, it will return the dictionary form of a word, which must be a valid word.\n",
    "\n",
    "\n",
    "However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.\n",
    "\n",
    "Eg: The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Assignment 1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
